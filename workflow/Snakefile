import os
import sys
import math
import datetime
from snakemake.utils import min_version

min_version("7.32.2")


# report: "report/workflow.rst"


# Configuration
configfile: "config/config.yaml"


# Variables
REFIDS = config["references"].keys()


wildcard_constraints:
    refid="|".join(REFIDS),


# Default rule
rule all:
    input:
        expand(
            "results/final/{refid}.adotto_TRregions_v1.0.bed",
            refid=REFIDS,
        ),


## General Rules --------------------------------------------------------------
# - rules for sorting, compressing, and indexing files
rule samtools_index:
    input:
        "{prefix}.fasta.gz",
    output:
        "{prefix}.fasta.gz.fai",
    log:
        "logs/samtools_index/{prefix}.log",
    wrapper:
        "v2.6.0/bio/samtools/faidx"


rule bedtools_sort_bed:
    input:
        in_file="{prefix}.bed",
        genome=lambda wildcards: f"results/{next((id for id in REFIDS if id in wildcards.prefix), 'UNKNOWN')}.fasta.gz.fai",
    output:
        "{prefix}_sorted.bed",
    log:
        "logs/sort_bed/{prefix}.log",
    wrapper:
        "v2.6.0/bio/bedtools/sort"


rule bgzip:
    input:
        "{prefix}.bed",
    output:
        "{prefix}.bed.gz",
    threads: 1
    log:
        "logs/bgzip/{prefix}.log",
    wrapper:
        "v2.6.0/bio/bgzip"


rule tabix:
    input:
        "{prefix}.bed.gz",
    output:
        "{prefix}.bed.gz.tbi",
    log:
        "logs/tabix/{prefix}.log",
    params:
        "-p bed",
    wrapper:
        "v2.6.0/bio/tabix/index"


rule create_genome_file:
    input:
        fai="{prefix}.fasta.gz.fai",
    output:
        genome="{prefix}.genome",
    log:
        "logs/create_genome_file/{prefix}.log",
    conda:
        "envs/download.yml"
    shell:
        """
        # Extract the first two columns (chromosome and size) from the .fai file
        cut -f 1,2 {input.fai} 1> {output.genome} 2> {log}
        """


## Downloading resources ------------------------------------------------------
# Rule to download the reference genome
rule download_reference:
    output:
        "results/references/{refid}.fasta.gz",
    params:
        refurl=lambda wildcards: config["references"][wildcards.refid]["REFURL"],
        expected_md5=lambda wildcards: config["references"][wildcards.refid]["REF_MD5"],
    log:
        "logs/download/{refid}_download_reference.log",
    resources:
        attempts=3,
    conda:
        "envs/download.yml"
    shell:
        """
        echo "[$(date)] Starting download of {output}" >> {log}
        curl -L {params.refurl} 1> {output} 2> {log}
        # Error handling
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error downloading {output}" >> {log}
            exit 1
        fi
        echo "{params.expected_md5}  {output}" | md5sum -c - &>> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] MD5 Checksum failed for {output}" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished download of {output}" >> {log}
        """


# Rule to download GIAB TR stratification
rule download_giab_tr:
    output:
        "results/annotations/{refid}.tr_regions.bed.gz",
    params:
        giabtrurl=lambda wildcards: config["references"][wildcards.refid]["GIABTRURL"],
        expected_md5=lambda wildcards: config["references"][wildcards.refid][
            "GIABTR_MD5"
        ],
    log:
        "logs/download/{refid}_download_giab_tr.log",
    resources:
        attempts=3,
    conda:
        "envs/download.yml"
    shell:
        """
        echo "[$(date)] Starting download of GIAB TR stratifications" >> {log}
        curl -L {params.giabtrurl} 1> {output} 2>> {log}
        # Error handling
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error downloading GIAB TR stratification" >> {log}
            exit 1
        fi
        echo "{params.expected_md5}  {output}" | md5sum -c - &>> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] MD5 Checksum failed for {output}" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished download of {output}" >> {log}
        """


## TR Region Stats and Pre-Processing -----------------------------------------
## Summary stats for region (span) size distribution
### https://github.com/ACEnglish/adotto/blob/main/regions/scripts/bed_stats.py
rule span_stats:
    input:
        bed="results/annotations/{refid}.tr_regions.bed.gz",
    output:
        stats="results/stats/{refid}.tr_regions_span_stats.txt",
    log:
        "logs/stats/{refid}.tr_regions_span_stats.log",
    conda:
        "envs/span_stats.yml"
    shell:
        """
        echo "[$(date)] Starting span stats computation" >> {log}
        zcat {input.bed} \
            | python workflow/scripts/bed_stats.py \
            1> {output.stats} 2> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error computing span stats" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished span stats computation" >> {log}
    """


## Filtering Regions
## - removing regions from extra chromosomes, regions lt 50 bp and gt 50kb
## - follows methods used to generate Adotto DB v1.0
## https://github.com/ACEnglish/adotto/blob/main/regions/scripts/merged_bed_filter.py
rule filter_regions:
    input:
        bed="results/annotations/{refid}.tr_regions.bed.gz",
    output:
        bed="results/filtered/{refid}.tr_regions_filtered.bed",
        stats="results/stats/{refid}.tr_regions_filter_stats.json",
    log:
        "logs/filtering/{refid}.tr_regions_filter_regions.log",
    conda:
        "envs/span_stats.yml"
    shell:
        """
        echo "[$(date)] Starting filtering of regions" >> {log}
        zcat {input.bed} \
            | python workflow/scripts/filter_tr_regions.py \
            {output.stats} 1> {output.bed} 2> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error during region filtering" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished filtering of regions" >> {log}
    """


rule bedtools_slop:
    input:
        "results/filtered/{refid}.tr_regions_filtered.bed",
        genome="results/{refid}.genome",
    output:
        "results/slopped/{refid}.tr_regions_filtered_slop25.bed",
    params:
        ## Genome file, tab-seperated file defining the length of every contig
        genome=lambda w, input: input[0],
        extra="-b 25",
    log:
        "logs/slop/{refid}.tr_regions_filtered_slop25.log",
    wrapper:
        "v2.6.0/bio/bedtools/slop"


## TRF Annotations ------------------------------------------------------------
# Rule to get repeat sequences using samtools
rule get_repeat_sequences:
    input:
        bed="results/slopped/{refid}.tr_regions_filtered_slop25.bed",
        ref="results/referene/{refid}.fasta.gz",
    output:
        "results/sequences/{refid}.tr_sequences.fasta",
    log:
        "logs/sequences/{refid}_get_repeat_sequences.log",
    conda:
        "envs/samtools.yml"
    shell:
        """
        echo "[$(date)] Starting extraction of repeat sequences" >> {log}
        awk '{{print $1 ":" $2 "-" $3}}' {input.bed} 2>> {log} \
            | samtools faidx {input.ref} -r - \
            1> {output} 2>> {log}
        echo "[$(date)] Finished extraction of repeat sequences" >> {log}
        """


# Rule to annotate sequences using trf
rule annotate_sequences:
    input:
        "results/sequences/{refid}.tr_sequences.fasta",
    output:
        "results/a{refid}.tandemrepeatfinder.txt",
    log:
        "logs/{refid}_annotate_sequences.log",
    conda:
        "envs/trf.yml"
    shell:
        "trf {input} 3 7 7 80 5 5 500 -h -ngs 1> {output} 2>{log}"


# Rule for TRF parsing
rule trf_parsing:
    input:
        "results/annotations/{refid}.tandemrepeatfinder.txt",
    output:
        "results/parsed/{refid}.trf_annos.bed",
    log:
        "logs/parsing/{refid}.trf_parsing.log",
    conda:
        "envs/trf_reformatter.yml"
    shell:
        """
        echo "[$(date)] Starting TRF parsing" >> {log}
        python workflow/scripts/trf_reformatter.py {input} {output} > {log}
        echo "[$(date)] Finished TRF parsing" >> {log}
        """


## TRF intersection
## - intersect trf_annos back to the input sources for QC
## - will need to modify for single source and replace/ fix hard coded paths
## https://github.com/ACEnglish/adotto/blob/main/regions/scripts/bed_intersection_stats.py
rule trf_intersection:
    input:
        tr="results/filtered/{refid}.tr_regions_filtered.bed",
        trannos="results/parsed/{refid}.trf_annos.bed",
    output:
        "results/intersection/{refid}.intersection.jl",
    log:
        "logs/intrersection/{refid}.trf_intersection.log",
    conda:
        "envs/annotation_improver.yml"
    shell:
        """
        echo "[$(date)] Starting TRF intersection" >> {log}
        python scripts/bed_intersection_stats.py {input.tr} {input.trannos} &> {log}
        echo "[$(date)] Finished TRF intersection" >> {log}
    """


## TODO - add rule to QC TRF annotations
## -- will need to fix hard coded paths and modify as necessary to work within snakemake
## https://github.com/ACEnglish/adotto/blob/main/regions/notebooks/analysis.ipynb


# Rule for identifying unannotated regions
rule unannotated_regions:
    input:
        regions="results/slopped/{refid}.tr_regions_filtered_slop25.bed",
        annotated="results/parsed/{refid}.trf_annos.bed",
    output:
        "results/unannotated/{refid}.unannotated_regions.bed",
    log:
        "logs/unannotated/{refid}_unannotated_regions.log",
    conda:
        "envs/bedtools.yml"
    shell:
        """
        echo "[$(date)] Starting identification of unannotated regions" >> {log}
        bedtools intersect -a {input.regions} -b {input.annotated} -c 2> {log}\
            | awk '$4 == 0' 1> {output} 2>> {log}
        echo "[$(date)] Finished identification of unannotated regions" >> {log}
        """


# Rule for intermediate annotations
rule intermediate_annotations:
    input:
        regions="results/{refid}.tr_regions_filtered_slop25.bed",
        regions_tbi="results/{refid}.tr_regions.bed.gz.tbi",
        annotations="results/{refid}.trf_annos_sorted.bed.gz",
        annotations_tbi="results/{refid}.trf_annos_sorted.bed.gz.tbi",
    output:
        bed="results/{refid}.candidate_v1.0.bed",
    log:
        "logs/{refid}_intermediate_annotations.log",
    conda:
        "envs/intermediate_annotations.yml"
    shell:
        """
        python workflow/scripts/tr_reganno_maker.py \
            {input.regions} {input.annotations}  \
            1> {output.bed} 2> {log}
        """


## RepeatMasker Annotations ---------------------------------------------------
# Rule for splitting fasta into multiple parts
rule split_fasta:
    input:
        fasta="results/seqeunces/{refid}.tr_sequences.fasta",
    output:
        expand(
            "results/split_fasta_parts/{{refid}}.part{part}.fasta",
            part=range(config["n_splits"]),
        ),
    params:
        n_parts=config["n_splits"],
        refid="{refid}",
        outdir=lambda w, output: os.path.dirname(output[0]) + "/",
    log:
        "logs/split_fasta/{refid}_split_fasta.log",
    conda:
        "envs/fasta_splitter.yml"
    shell:
        """
        echo "[$(date)] Starting fasta splitting" >> {log}
        python workflow/scripts/fasta_splitter.py \
            --fasta_path {input.fasta} \
            --n_parts {params.n_parts} \
            --output_dir {params.outdir} \
            --refid {params.refid} \
            &>> {log}
        echo "[$(date)] Finished fasta splitting" >> {log}
        """


# Rule for creating RepeatMasker annotations
rule repeatmasker_annotations:
    input:
        split_fasta="results/split_fasta_parts/{refid}.part{part}.fasta",
    output:
        annotated="results/repeatmasker_annotated/{refid}/{refid}.part{part}.fasta.out",
    log:
        "logs/repeat_masker/{refid}_part{part}.log",
    params:
        outdir="results/repeatmasker_annotated/{refid}/",
    threads: config["rm_threads"]
    conda:
        "envs/repeatmasker.yml"
    shell:
        """
        echo "[$(date)] Starting RepeatMasker annotation" >> {log}
        RepeatMasker \
            -pa {threads} \
            -qq -e hmmer -species human -lcambig -nocut -div 50 -no_id \
            -dir {params.outdir} \
            -s {input.split_fasta} &>> {log}
        echo "[$(date)] Finished RepeatMasker annotation" >> {log}
        """


# Rule to convert RepeatMasker Output to joblib
rule convert_repeatmasker_to_joblib:
    input:
        rmout_files=expand(
            "results/repeatmasker_annotated/{{refid}}/{{refid}}.part{part}.fasta.out",
            part=range(config["n_splits"]),
        ),
    output:
        "results/joblib/{refid}.repmask.jl",
    log:
        "logs/joblib/{refid}_convert_repeatmasker.log",
    conda:
        "envs/repmask_to_joblib.yml"
    shell:
        """
        echo "[$(date)] Starting conversion of RepeatMasker output to joblib" >> {log}
        python workflow/scripts/repmask_to_joblib.py {input.rmout_files} 2> {log}
        mv repmask_results.dict.jl {output}
        cho "[$(date)] Finished conversion to joblib" >> {log}
        """


## Building Adotto Formatted Tandem Repeat Database ---------------------------
# Rule for improving annotations
rule improve_annotations:
    input:
        candidate="results/{refid}.candidate_v1.0_sorted.bed.gz",
        reference="results/{refid}.fasta.gz",
        repmask="results/joblib/{refid}.repmask.jl",
    output:
        "results/final/{refid}.adotto_TRregions_v1.0.bed",
    log:
        "logs/final/{refid}_improve_annotations.log",
    conda:
        "envs/annotation_improver.yml"
    shell:
        """
        echo "[$(date)] Starting annotation improvement" >> {log}
        python workflow/scripts/annotation_improver.py \
            {input.candidate} \
            {input.reference} \
            {input.repmask} \
            1> {output} 2>> {log}
        echo "[$(date)] Finished annotation improvement" >> {log}
        """
