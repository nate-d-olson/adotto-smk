"""
================================================================================
Snakefile for Tandem Repeat Annotation Pipeline
================================================================================

Description:
    This Snakefile defines a workflow for generating a well-annotated BED file 
    of tandem repeats across multiple genomes. The methods employed here were
    developed by AC English for Adotto (https://github.com/ACEnglish/adotto/tree/main)

Author:
    ND Olson - nolson@nist.gov

Creation Date:
    October 12, 2023

Requirements:
    - Snakemake v7.32.2 or higher
    - Conda environments as defined in the envs/ directory
    - External Python scripts in the workflow/scripts/ directory

Usage:
    snakemake --use-conda --cores [number of cores]

Notes:
    Ensure the config.yaml is properly set up before executing the pipeline. 
    For more detailed documentation, please refer to the accompanying README.md.

================================================================================
"""


import os
import sys
import math
import datetime
from snakemake.utils import min_version

min_version("7.32.2")


# report: "report/workflow.rst"


# Configuration
configfile: "config/config.yaml"


# Variables
REFIDS = config["references"].keys()


wildcard_constraints:
    refid="|".join(REFIDS),


# Default rule
rule all:
    input:
        expand(
            "results/final/{refid}.adotto_TRregions_v1.0.bed.gz",
            refid=REFIDS,
        ),


## General Rules --------------------------------------------------------------
# - rules for sorting, compressing, and indexing files
rule samtools_index:
    input:
        "{prefix}.fasta.gz",
    output:
        "{prefix}.fasta.gz.fai",
    log:
        "logs/samtools_index/{prefix}.log",
    wrapper:
        "v2.6.0/bio/samtools/faidx"


rule bedtools_sort_bed:
    input:
        in_file="{prefix}.bed",
        genome=lambda wildcards: f"results/references/{next((id for id in REFIDS if id in wildcards.prefix), 'UNKNOWN')}.fasta.gz.fai",
    output:
        "{prefix}_sorted.bed",
    log:
        "logs/sort_bed/{prefix}.log",
    wrapper:
        "v2.6.0/bio/bedtools/sort"


rule bgzip:
    input:
        "{prefix}.bed",
    output:
        "{prefix}.bed.gz",
    threads: 1
    log:
        "logs/bgzip/{prefix}.log",
    wrapper:
        "v2.6.0/bio/bgzip"


rule tabix:
    input:
        "{prefix}.bed.gz",
    output:
        "{prefix}.bed.gz.tbi",
    log:
        "logs/tabix/{prefix}.log",
    params:
        "-p bed",
    wrapper:
        "v2.6.0/bio/tabix/index"


rule create_genome_file:
    input:
        fai="{prefix}.fasta.gz.fai",
    output:
        genome="{prefix}.genome",
    log:
        "logs/create_genome_file/{prefix}.log",
    conda:
        "envs/download.yml"
    shell:
        """
        # Extract the first two columns (chromosome and size) from the .fai file
        cut -f 1,2 {input.fai} 1> {output.genome} 2> {log}
        """


## Downloading resources ------------------------------------------------------
# Rule to download the reference genome
rule download_reference:
    output:
        "results/references/{refid}.fasta.gz",
    params:
        refurl=lambda wildcards: config["references"][wildcards.refid]["REFURL"],
        expected_md5=lambda wildcards: config["references"][wildcards.refid]["REF_MD5"],
    log:
        "logs/download_ref/{refid}.log",
    resources:
        attempts=3,
    conda:
        "envs/download.yml"
    shell:
        """
        echo "[$(date)] Starting download of {output}" >> {log}
        curl -L {params.refurl} 1> {output} 2> {log}
        # Error handling
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error downloading {output}" >> {log}
            exit 1
        fi
        echo "{params.expected_md5}  {output}" | md5sum -c - &>> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] MD5 Checksum failed for {output}" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished download of {output}" >> {log}
        """


# Rule to download GIAB TR stratification
rule download_giab_tr:
    output:
        "results/regions/{refid}/{refid}.tr_regions.bed.gz",
    params:
        giabtrurl=lambda wildcards: config["references"][wildcards.refid]["GIABTRURL"],
        expected_md5=lambda wildcards: config["references"][wildcards.refid][
            "GIABTR_MD5"
        ],
    log:
        "logs/download_tr/{refid}.log",
    resources:
        attempts=3,
    conda:
        "envs/download.yml"
    shell:
        """
        echo "[$(date)] Starting download of GIAB TR stratifications" >> {log}
        curl -L {params.giabtrurl} 1> {output} 2>> {log}
        # Error handling
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error downloading GIAB TR stratification" >> {log}
            exit 1
        fi
        echo "{params.expected_md5}  {output}" | md5sum -c - &>> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] MD5 Checksum failed for {output}" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished download of {output}" >> {log}
        """


## TR Region Stats and Pre-Processing -----------------------------------------
## Summary stats for region (span) size distribution
### https://github.com/ACEnglish/adotto/blob/main/regions/scripts/bed_stats.py
rule span_stats:
    input:
        bed="results/regions/{refid}/{refid}.tr_regions.bed.gz",
    output:
        stats="results/regions/{refid}/{refid}.tr_regions_span_stats.txt",
    log:
        "logs/span_stats/{refid}.log",
    conda:
        "envs/span_stats.yml"
    shell:
        """
        echo "[$(date)] Starting span stats computation" >> {log}
        zcat {input.bed} \
            | python workflow/scripts/span_stats.py \
            1> {output.stats} 2> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error computing span stats" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished span stats computation" >> {log}
    """


## Filtering Regions
## - removing regions from extra chromosomes, regions lt 50 bp and gt 50kb
## - follows methods used to generate Adotto DB v1.0
## https://github.com/ACEnglish/adotto/blob/main/regions/scripts/merged_bed_filter.py
rule filter_regions:
    input:
        bed="results/regions/{refid}/{refid}.tr_regions.bed.gz",
    output:
        bed="results/regions/{refid}/{refid}.tr_regions_filt.bed",
        stats="results/regions/{refid}/{refid}.tr_regions_filt_stats.json",
    log:
        "logs/filter_region/{refid}.log",
    conda:
        "envs/span_stats.yml"
    shell:
        """
        echo "[$(date)] Starting filtering of regions" >> {log}
        zcat {input.bed} \
            | python workflow/scripts/filter_tr_regions.py \
            {output.stats} 1> {output.bed} 2> {log}
        if [ $? -ne 0 ]; then
            echo "[$(date)] Error during region filtering" >> {log}
            exit 1
        fi
        echo "[$(date)] Finished filtering of regions" >> {log}
    """


rule bedtools_slop:
    input:
        "results/regions/{refid}/{refid}.tr_regions_filt.bed",
        genome="results/references/{refid}.genome",
    output:
        "results/regions/{refid}/{refid}.tr_regions_filt_slop25.bed",
    params:
        ## Genome file, tab-seperated file defining the length of every contig
        genome=lambda w, input: input[0],
        extra="-b 25",
    log:
        "logs/bedtools_slop/{refid}.log",
    wrapper:
        "v2.6.0/bio/bedtools/slop"


## Summary Stats for regions
rule get_region_stats:
    input:
        "TODO - workout",
    output:
        "results/regions/{refid}/{refid}.tr_regions_filt_slop25_regions_stats.txt",
    log:
        "log/get_region_stats/{refid}.log",
    conda:
        "envs/span_stats.yml"
    shell:
        """
    python scripts/consolidate_stats.py {input} > {output}
    """


# Rule to get repeat sequences using samtools
rule get_repeat_sequences:
    input:
        bed="results/regions/{refid}/{refid}.tr_regions_filt_slop25.bed",
        ref="results/references/{refid}.fasta.gz",
    output:
        "results/regions/{refid}/{refid}.tr_sequences.fasta",
    log:
        "logs/get_repeat_sequences/{refid}.log",
    conda:
        "envs/samtools.yml"
    shell:
        """
        echo "[$(date)] Starting extraction of repeat sequences" >> {log}
        awk '{{print $1 ":" $2 "-" $3}}' {input.bed} 2>> {log} \
            | samtools faidx {input.ref} -r - \
            1> {output} 2>> {log}
        echo "[$(date)] Finished extraction of repeat sequences" >> {log}
        """


# Rule for splitting fasta into multiple parts
rule split_fasta:
    input:
        fasta="results/regions/{refid}/{refid}.tr_sequences.fasta",
    output:
        expand(
            "results/regions/{{refid}}/split_fasta/{{refid}}.part{part}.fasta",
            part=range(config["n_splits"]),
        ),
    params:
        n_parts=config["n_splits"],
        refid="{refid}",
        outdir=lambda w, output: os.path.dirname(output[0]) + "/",
    log:
        "logs/split_fasta/{refid}.log",
    conda:
        "envs/fasta_splitter.yml"
    shell:
        """
        echo "[$(date)] Starting fasta splitting" >> {log}
        python workflow/scripts/fasta_splitter.py \
            --fasta_path {input.fasta} \
            --n_parts {params.n_parts} \
            --output_dir {params.outdir} \
            --refid {params.refid} \
            &>> {log}
        echo "[$(date)] Finished fasta splitting" >> {log}
        """


## TRF Annotations ------------------------------------------------------------


# Rule to annotate sequences using trf
rule annotate_seqs_wtrf:
    input:
        "results/regions/{refid}/{refid}.tr_sequences.fasta",
    output:
        "results/annotations/trf/{refid}/{refid}.tandemrepeatfinder.txt",
    log:
        "logs/annotate_seqs_wtrf/{refid}.log",
    conda:
        "envs/trf.yml"
    shell:
        """
        echo "[$(date)] Starting TRF" >> {log}
        trf {input} 3 7 7 80 5 5 500 -h -ngs 1> {output} 2>>{log}
        echo "[$(date)] Finished TRF" >> {log}
        """


# Rule for TRF reformatting
rule trf_reformatter:
    input:
        "results/annotations/trf/{refid}/{refid}.tandemrepeatfinder.txt",
    output:
        "results/annotations/trf/{refid}/{refid}.trf_annotations.bed",
        "results/annotations/trf/{refid}/{refid}.trf_annotations.bed.jl",
    log:
        "logs/trf_parsing/{refid}.log",
    conda:
        "envs/trf_reformatter.yml"
    shell:
        """
        echo "[$(date)] Starting TRF parsing" >> {log}
        python workflow/scripts/trf_reformatter.py {input} {output} > {log}
        echo "[$(date)] Finished TRF parsing" >> {log}
        """


## TRF intersection
## - intersect trf_annos back to the input sources for QC
## - will need to modify for single source and replace/ fix hard coded paths
## https://github.com/ACEnglish/adotto/blob/main/regions/scripts/bed_intersection_stats.py
rule trf_intersection:
    input:
        tr="results/regions/{refid}/{refid}.tr_regions_filt.bed",
        trannos="results/annotations/trf/{refid}/{refid}.trf_annotations.bed",
    output:
        "results/annotations/trf/{refid}/{refid}.intersection.jl",
    log:
        "logs/trf_intrersection/{refid}.trf_intersection.log",
    conda:
        "envs/annotation_improver.yml"
    shell:
        """
        echo "[$(date)] Starting TRF intersection" >> {log}
        python scripts/trf_intersection.py {input.tr} {input.trannos} &> {log}
        echo "[$(date)] Finished TRF intersection" >> {log}
    """


## TODO - add rule to QC TRF annotations
## -- will need to fix hard coded paths and modify as necessary to work within snakemake
## https://github.com/ACEnglish/adotto/blob/main/regions/notebooks/analysis.ipynb
rule trf_qc:
    input:
        region_stats="path/to/your/data/{refid}.region_stats.txt",
        regions_bed="results/regions/{refid}/{refid}.tr_regions_filt_slop25.bed",
        tr_anno_jl="results/annotations/trf/{refid}/{refid}.trf_annotations.bed.jl",
        intersections_jl="results/annotations/trf/{refid}/{refid}.intersection.jl",
    output:
        report="results/annotations/trf/{refid}/{refid}.trf_qc_report.html",
    log:
        "logs/trf_qc/{refid}.log",
    conda:
        "envs/trf_gz.log"
    params:
        notebook="workflow/notebooks/trf_qc.ipynb",
    shell:
        """
        papermill {params.notebook} {output.report} -p input_data_path {input.data}
        """


# Rule for identifying unannotated regions
rule unannotated_regions:
    input:
        regions="results/regions/{refid}/{refid}.tr_regions_filt_slop25.bed",
        annotated="results/annotations/trf/{refid}/{refid}.trf_annotations.bed",
    output:
        "results/annotations/trf/{refid}/{refid}.unannotated_regions.bed",
    log:
        "logs/unannotated_regions/{refid}.log",
    conda:
        "envs/bedtools.yml"
    shell:
        """
        echo "[$(date)] Starting identification of unannotated regions" >> {log}
        bedtools intersect -a {input.regions} -b {input.annotated} -c 2> {log}\
            | awk '$4 == 0' 1> {output} 2>> {log}
        echo "[$(date)] Finished identification of unannotated regions" >> {log}
        """


# Rule for intermediate annotations
rule intermediate_annotations:
    input:
        regions="results/regions/{refid}/{refid}.tr_regions_filt_slop25.bed.gz",
        regions_tbi="results/regions/{refid}/{refid}.tr_regions_filt_slop25.bed.gz.tbi",
        annotations="results/annotations/trf/{refid}/{refid}.trf_annotations_sorted.bed.gz",
        annotations_tbi="results/annotations/trf/{refid}/{refid}.trf_annotations_sorted.bed.gz.tbi",
    output:
        bed="results/annotations/{refid}.candidate_v1.0.bed",
    log:
        "logs/intermediate_annotations/{refid}.log",
    conda:
        "envs/intermediate_annotations.yml"
    shell:
        """
        python workflow/scripts/intermediate_annotations.py \
            {input.regions} {input.annotations}  \
            1> {output.bed} 2> {log}
        """


## RepeatMasker Annotations ---------------------------------------------------


# Rule for creating RepeatMasker annotations
rule repeatmasker_annotations:
    input:
        split_fasta="results/regions/{refid}/split_fasta/{refid}.part{part}.fasta",
    output:
        annotated="results/annotations/repeatmasker/{refid}/{refid}.part{part}.fasta.out",
    log:
        "logs/repeatmasker/{refid}/part{part}.log",
    params:
        outdir="results/annotations/repeatmasker/{refid}/",
    threads: config["rm_threads"]
    conda:
        "envs/repeatmasker.yml"
    shell:
        """
        echo "[$(date)] Starting RepeatMasker annotation" >> {log}
        RepeatMasker \
            -pa {threads} \
            -qq -e hmmer -species human -lcambig -nocut -div 50 -no_id \
            -dir {params.outdir} \
            -s {input.split_fasta} &>> {log}
        echo "[$(date)] Finished RepeatMasker annotation" >> {log}
        """


# Rule to convert RepeatMasker Output to joblib
rule convert_repeatmasker_to_joblib:
    input:
        rmout_files=expand(
            "results/annotations/repeatmasker/{{refid}}/{{refid}}.part{part}.fasta.out",
            part=range(config["n_splits"]),
        ),
    output:
        "results/annotations/repeatmasker/{refid}.repmask.jl",
    log:
        "logs/convert_repeatmasker_to_joblib/{refid}.log",
    conda:
        "envs/repmask_to_joblib.yml"
    shell:
        """
        echo "[$(date)] Starting conversion of RepeatMasker output to joblib" > {log}
        python workflow/scripts/repmask_to_joblib.py {output} {input.rmout_files} &>> {log}
        echo "[$(date)] Finished conversion to joblib" >> {log}
        """


## Building Adotto Formatted Tandem Repeat Database ---------------------------
# Rule for improving annotations
rule improve_annotations:
    input:
        candidate="results/annotations/{refid}.candidate_v1.0_sorted.bed.gz",
        reference="results/references/{refid}.fasta.gz",
        repmask="results/annotations/repeatmasker/{refid}.repmask.jl",
    output:
        "results/final/{refid}.adotto_TRregions_v1.0.bed",
    log:
        "logs/improve_annotations/{refid}.log",
    conda:
        "envs/annotation_improver.yml"
    shell:
        """
        echo "[$(date)] Starting annotation improvement" >> {log}
        python workflow/scripts/annotation_improver.py \
            {input.candidate} \
            {input.reference} \
            {input.repmask} \
            1> {output} 2>> {log}
        echo "[$(date)] Finished annotation improvement" >> {log}
        """
